{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "752aa2cd",
   "metadata": {},
   "source": [
    "#### IMPORTING DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65d4ddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703559f7",
   "metadata": {},
   "source": [
    "#### Display Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07e8750a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/match_0_kill.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/match_0_kill.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/match_0_kill.csv'"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"data/match_0_kill.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857686ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6b7f69",
   "metadata": {},
   "source": [
    "#### Join operation over the GSR Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6c59c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):  # Assuming 5 players, indexed from 0 to 4\n",
    "    gsr_df = pd.read_csv(f'gsr_{i}.csv')\n",
    "    gsr_df['time'] = gsr_df['time'].astype(str).str.replace('.0', '', regex=False).astype(int)\n",
    "    gsr_df.rename(columns={'gsr_value': f'gsr_player_{i}'}, inplace=True)\n",
    "    match_kill_df = pd.merge(df, gsr_df, on='time', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b4ac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_kill_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9272ad5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for player_id in range(5):  # Assuming player IDs are from 0 to 4\n",
    "    # Filter data for the current player\n",
    "    player_data = match_kill_df[match_kill_df['player_id'] == player_id]\n",
    "    \n",
    "    # Plot setup\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(player_data['time'], player_data['gsr'], label=f'GSR Player {player_id}')\n",
    "    \n",
    "    # Mark kills\n",
    "    kill_times = player_data[player_data['kill'] == 1]['time']\n",
    "    kill_gsr = player_data[player_data['kill'] == 1]['gsr']\n",
    "    plt.scatter(kill_times, kill_gsr, color='black', label='Kill')\n",
    "    \n",
    "    # Mark assists\n",
    "    assist_times = player_data[player_data['assist'] == 1]['time']\n",
    "    assist_gsr = player_data[player_data['assist'] == 1]['gsr']\n",
    "    plt.scatter(assist_times, assist_gsr, color='blue', label='Assist')\n",
    "    \n",
    "    # Finalizing plot\n",
    "    plt.title(f'GSR over Time for Player {player_id}')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('GSR Value')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204f3099",
   "metadata": {},
   "source": [
    "## Relating the EEG Values with the hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ae88a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('match_0_kill.csv')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Filtering data for Player 1\n",
    "player_data = data_df[data_df['player_id'] == 1]\n",
    "\n",
    "# Determine scaling factors to visually align the different metrics on the graphs\n",
    "gsr_max = player_data['gsr'].max()\n",
    "heart_rate_scaled_max = player_data['heart_rate'].max() / gsr_max\n",
    "\n",
    "# Additional metrics to plot\n",
    "additional_metrics = ['Engagement', 'Excitement', 'Stress', 'Relaxation', 'Interest', 'Focus']\n",
    "\n",
    "# Create subplots for the base graph and additional metrics\n",
    "nrows = len(additional_metrics) + 1  # Additional rows for the base graph\n",
    "fig, axs = plt.subplots(nrows, 1, figsize=(15, 5 * nrows), sharex=True)\n",
    "\n",
    "# Base graph for GSR, heart rate, kill, and assist\n",
    "# Plot GSR values on the base graph (axs[0])\n",
    "axs[0].plot(player_data['time'], player_data['gsr'], label='GSR', color='green')\n",
    "# Scale and plot Heart rate curve in red on the base graph (axs[0])\n",
    "axs[0].plot(player_data['time'], player_data['heart_rate'] * heart_rate_scaled_max, label='Heart Rate (scaled)', color='red')\n",
    "# Mark kills and assists on the base graph (axs[0])\n",
    "axs[0].scatter(player_data[player_data['kill'] == 1]['time'], \n",
    "               player_data[player_data['kill'] == 1]['gsr'], \n",
    "               label='Kill', color='black', zorder=5)\n",
    "axs[0].scatter(player_data[player_data['assist'] == 1]['time'], \n",
    "               player_data[player_data['assist'] == 1]['gsr'], \n",
    "               label='Assist', color='blue', zorder=5)\n",
    "axs[0].legend(loc='upper left')\n",
    "axs[0].set_ylabel('GSR & Scaled Heart Rate')\n",
    "axs[0].set_title('Player 1 - GSR, Heart Rate with Kills and Assists Over Time')\n",
    "\n",
    "# Plot each additional metric in a separate subplot\n",
    "for i, metric in enumerate(additional_metrics):\n",
    "    # Plot the additional metric on the ith+1 subplot\n",
    "    axs[i+1].plot(player_data['time'], player_data[metric], label=metric.capitalize(), color='blue')\n",
    "    # Mark kills and assists on the ith+1 subplot\n",
    "    axs[i+1].scatter(player_data[player_data['kill'] == 1]['time'], \n",
    "                     player_data[player_data['kill'] == 1][metric], \n",
    "                     label='Kill', color='black', zorder=5)\n",
    "    axs[i+1].scatter(player_data[player_data['assist'] == 1]['time'], \n",
    "                     player_data[player_data['assist'] == 1][metric], \n",
    "                     label='Assist', color='blue', zorder=5)\n",
    "    axs[i+1].set_ylabel(metric.capitalize())\n",
    "    axs[i+1].legend(loc='upper left')\n",
    "\n",
    "# Show the plot\n",
    "plt.xlabel('Time')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2457aa25",
   "metadata": {},
   "source": [
    "## Improving the Scaling of Heart rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64ac9cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    " # Replace with your actual file path\n",
    "\n",
    "# Filtering data for Player 1\n",
    "player_data = data_df[data_df['player_id'] == 1]\n",
    "\n",
    "# Determine a scaling factor for heart rate values to bring them closer to the GSR value range for the first graph\n",
    "scaling_factor_for_first_graph = player_data['gsr'].max() / player_data['heart_rate'].max()\n",
    "\n",
    "# Additional metrics to plot\n",
    "additional_metrics = ['Engagement', 'Excitement', 'Stress', 'Relaxation', 'Interest', 'Focus']\n",
    "\n",
    "# Create subplots for the base graph and additional metrics\n",
    "nrows = len(additional_metrics)  # Number of additional metrics\n",
    "fig, axs = plt.subplots(nrows + 1, 1, figsize=(15, 5 * (nrows + 1)), sharex=True)\n",
    "\n",
    "# Plot the base graph of GSR and scaled heart rate\n",
    "# Plot GSR values and scaled Heart rate curve on the first subplot\n",
    "axs[0].plot(player_data['time'], player_data['gsr'], label='GSR', color='green')\n",
    "axs[0].plot(player_data['time'], player_data['heart_rate'] * scaling_factor_for_first_graph, label='Heart Rate (scaled)', color='red')\n",
    "# Mark kills and assists on the first subplot\n",
    "axs[0].scatter(player_data[player_data['kill'] == 1]['time'], \n",
    "               player_data[player_data['kill'] == 1]['gsr'], \n",
    "               label='Kill', color='black', zorder=5)\n",
    "axs[0].scatter(player_data[player_data['assist'] == 1]['time'], \n",
    "               player_data[player_data['assist'] == 1]['gsr'], \n",
    "               label='Assist', color='blue', zorder=5)\n",
    "axs[0].legend(loc='upper left')\n",
    "axs[0].set_ylabel('GSR & Scaled Heart Rate')\n",
    "axs[0].set_title('Player 1 - GSR and Scaled Heart Rate with Kills and Assists Over Time')\n",
    "\n",
    "# Plot each additional metric in a separate subplot below the base graph\n",
    "for i, metric in enumerate(additional_metrics):\n",
    "    # Plot the additional metric on the ith+1 subplot\n",
    "    axs[i+1].plot(player_data['time'], player_data[metric], label=metric, color='blue')\n",
    "    # Mark kills and assists on the ith+1 subplot \n",
    "    axs[i+1].scatter(player_data[player_data['kill'] == 1]['time'], \n",
    "                     player_data[player_data['kill'] == 1][metric], \n",
    "                     label='Kill', color='black', zorder=5)\n",
    "    axs[i+1].scatter(player_data[player_data['assist'] == 1]['time'], \n",
    "                     player_data[player_data['assist'] == 1][metric], \n",
    "                     label='Assist', color='blue', zorder=5)\n",
    "    axs[i+1].set_ylabel(metric)\n",
    "    axs[i+1].legend(loc='upper left')\n",
    "\n",
    "# Show the plot\n",
    "plt.xlabel('Time')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ab14a",
   "metadata": {},
   "source": [
    "## Tonic and Phasic GSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d101347e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "# Load the GSR data from a CSV file\n",
    "# Make sure to update the file path and column name according to your dataset\n",
    "df = pd.read_csv('match_0_kill.csv')\n",
    "\n",
    "# Filter the DataFrame for Player 1 only\n",
    "df_player1 = df[df['player_id'] == 1]\n",
    "\n",
    "# Update the sampling rate (fs) according to your data's sampling frequency\n",
    "fs = 1.0  # The sampling frequency in Hz\n",
    "\n",
    "# Try adjusting the lowcut frequency to better separate the tonic component\n",
    "lowcut = 0.05  # Low cut frequency for the tonic component, adjust this as needed\n",
    "\n",
    "# Butterworth filter for signal processing\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "# Ensure 'gsr' column exists in df_player1 before applying the filter\n",
    "if 'gsr' in df_player1.columns:\n",
    "    # Apply the low-pass filter to get the tonic component for Player 1\n",
    "    tonic = butter_lowpass_filter(df_player1['gsr'], lowcut, fs, order=6)\n",
    "\n",
    "    # The phasic component is the original signal minus the tonic component for Player 1\n",
    "    phasic = df_player1['gsr'] - tonic\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Original GSR data for Player 1\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(df_player1['gsr'], label='Original GSR')\n",
    "    plt.title('Original GSR Data (Player 1)')\n",
    "    plt.legend()\n",
    "\n",
    "    # Tonic Component for Player 1\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(tonic, label='Tonic Component', color='orange')\n",
    "    plt.title('Tonic Component (Player 1)')\n",
    "    plt.legend()\n",
    "\n",
    "    # Phasic Component for Player 1\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(phasic, label='Phasic Component', color='green')\n",
    "    plt.title('Phasic Component (Player 1)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"The 'gsr' column is missing from the dataset for Player 1.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f51756",
   "metadata": {},
   "source": [
    "## 1\n",
    "### Tonic Component Graph: By examining the trend in the tonic component, you can assess how the participant's baseline arousal level changes over time. An increasing trend might suggest growing tension or stress, while a decreasing trend could indicate relaxation.\n",
    "## 2\n",
    "### Phasic Component Graph: Looking at the peaks in the phasic component can help you identify moments of acute response. The presence, frequency, and magnitude of these peaks can be linked back to specific events, offering insights into the participant's reactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c108eb49",
   "metadata": {},
   "source": [
    "## Relating the Tonic and Phasic GSR data to Kills and Assists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0f4394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "# Load the GSR data from a CSV file\n",
    "df = pd.read_csv('match_0_kill.csv')\n",
    "\n",
    "# Assuming 'time' column exists and is in the correct format for plotting\n",
    "# Convert 'time' to datetime format if it's not already\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Filter the DataFrame for Player 1 only\n",
    "df_player1 = df[df['player_id'] == 1]\n",
    "\n",
    "# Update the sampling rate (fs) according to your data's sampling frequency\n",
    "fs = 1.0  # The sampling frequency in Hz, adjust this based on your data\n",
    "\n",
    "# Try adjusting the lowcut frequency to better separate the tonic component\n",
    "lowcut = 0.05  # Low cut frequency for the tonic component, adjust this as needed\n",
    "\n",
    "# Butterworth filter for signal processing\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "# Check if 'gsr' column exists in df_player1 before applying the filter\n",
    "if 'gsr' in df_player1.columns:\n",
    "    # Apply the low-pass filter to get the tonic component for Player 1\n",
    "    tonic = butter_lowpass_filter(df_player1['gsr'], lowcut, fs, order=6)\n",
    "\n",
    "    # The phasic component is the original signal minus the tonic component for Player 1\n",
    "    phasic = df_player1['gsr'] - tonic\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Original GSR data for Player 1 with time, kills, and assists\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(df_player1['time'], df_player1['gsr'], label='Original GSR')\n",
    "    # Plotting kill and assist as scatter plots on the GSR graph\n",
    "    # We use np.max(df_player1['gsr']) to scale the kill and assist markers\n",
    "    plt.scatter(df_player1[df_player1['kill'] > 0]['time'], [np.max(df_player1['gsr'])] * len(df_player1[df_player1['kill'] > 0]), label='Kill', color='red')\n",
    "    plt.scatter(df_player1[df_player1['assist'] > 0]['time'], [np.max(df_player1['gsr']) * 0.95] * len(df_player1[df_player1['assist'] > 0]), label='Assist', color='blue')\n",
    "    plt.title('Original GSR Data (Player 1)')\n",
    "    plt.legend()\n",
    "\n",
    "    # Tonic Component for Player 1 with time\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(df_player1['time'], tonic, label='Tonic Component', color='orange')\n",
    "    plt.title('Tonic Component (Player 1)')\n",
    "    plt.legend()\n",
    "\n",
    "    # Phasic Component for Player 1 with time\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(df_player1['time'], phasic, label='Phasic Component', color='green')\n",
    "    plt.title('Phasic Component (Player 1)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"The 'gsr' column is missing from the dataset for Player 1.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396e389",
   "metadata": {},
   "source": [
    "## Correlating the Kills and Assists and also scaling the time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e853fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "# Load the GSR data from a CSV file\n",
    "df = pd.read_csv('match_0_kill.csv')\n",
    "\n",
    "# Filter the DataFrame for Player 1 only\n",
    "df_player1 = df[df['player_id'] == 1]\n",
    "\n",
    "# Calculate elapsed time in seconds from the start\n",
    "elapsed_time = df_player1['time'] - df_player1['time'].iloc[0]\n",
    "\n",
    "# Define the sampling rate and the low cut frequency for the tonic component\n",
    "fs = 1.0  # Adjust based on your data\n",
    "lowcut = 0.05\n",
    "\n",
    "# Define Butterworth filter for signal processing\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "if 'gsr' in df_player1.columns:\n",
    "    # Apply the low-pass filter to obtain the tonic component\n",
    "    tonic = butter_lowpass_filter(df_player1['gsr'], lowcut, fs, order=6)\n",
    "\n",
    "    # Calculate the phasic component as the difference between the original and tonic components\n",
    "    phasic = df_player1['gsr'] - tonic\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Plot original GSR data\n",
    "    ax1 = plt.subplot(3, 1, 1)\n",
    "    ax1.plot(elapsed_time, df_player1['gsr'], label='Original GSR')\n",
    "    ax1.scatter(elapsed_time[df_player1['kill'] > 0], [np.max(df_player1['gsr'])] * len(df_player1[df_player1['kill'] > 0]), label='Kill', color='red')\n",
    "    ax1.scatter(elapsed_time[df_player1['assist'] > 0], [np.max(df_player1['gsr']) * 0.95] * len(df_player1[df_player1['assist'] > 0]), label='Assist', color='blue')\n",
    "    ax1.set_title('Original GSR Data (Player 1)')\n",
    "    ax1.set_xlabel('Time (seconds)')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot tonic component and mark kill and assist events\n",
    "    ax2 = plt.subplot(3, 1, 2, sharex=ax1)\n",
    "    ax2.plot(elapsed_time, tonic, label='Tonic Component', color='orange')\n",
    "    ax2.scatter(elapsed_time[df_player1['kill'] > 0], [np.max(tonic)] * len(df_player1[df_player1['kill'] > 0]), label='Kill', color='red')\n",
    "    ax2.scatter(elapsed_time[df_player1['assist'] > 0], [np.max(tonic) * 0.95] * len(df_player1[df_player1['assist'] > 0]), label='Assist', color='blue')\n",
    "    ax2.set_title('Tonic Component (Player 1)')\n",
    "    ax2.set_xlabel('Time (seconds)')\n",
    "    ax2.legend()\n",
    "\n",
    "    # Plot phasic component and mark kill and assist events\n",
    "    ax3 = plt.subplot(3, 1, 3, sharex=ax1)\n",
    "    ax3.plot(elapsed_time, phasic, label='Phasic Component', color='green')\n",
    "    ax3.scatter(elapsed_time[df_player1['kill'] > 0], [np.max(phasic)] * len(df_player1[df_player1['kill'] > 0]), label='Kill', color='red')\n",
    "    ax3.scatter(elapsed_time[df_player1['assist'] > 0], [np.max(phasic) * 0.95] * len(df_player1[df_player1['assist'] > 0]), label='Assist', color='blue')\n",
    "    ax3.set_title('Phasic Component (Player 1)')\n",
    "    ax3.set_xlabel('Time (seconds)')\n",
    "    ax3.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"The 'gsr' column is missing from the dataset for Player 1.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45a2780",
   "metadata": {},
   "source": [
    "## 1\n",
    "Tonic Component Graph: By examining the trend in the tonic component, you can assess how the participant's baseline arousal level changes over time. An increasing trend might suggest growing tension or stress, while a decreasing trend could indicate relaxation.\n",
    "## 2\n",
    "Phasic Component Graph: Looking at the peaks in the phasic component can help you identify moments of acute response. The presence, frequency, and magnitude of these peaks can be linked back to specific events, offering insights into the participant's reactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48299a9",
   "metadata": {},
   "source": [
    "## Player 1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495189b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_player1 = df[df['player_id'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedf967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_player1.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fd8f10",
   "metadata": {},
   "source": [
    "- Theta (4-7 Hz): Increased theta activity is often associated with states of relaxation, meditativeness, and drowsiness.\n",
    "- Alpha (8-13 Hz): Alpha waves are associated with a state of wakeful relaxation with closed eyes and can indicate relaxation and decreased cognitive load.\n",
    "- Beta (13-30 Hz): Higher beta activity can be a sign of active, busy, or anxious thinking and active concentration.\n",
    "- Gamma (>30 Hz): Gamma waves are related to higher mental activity, including perception, problem-solving, and consciousness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3503ec46",
   "metadata": {},
   "source": [
    "## Statistical Analysis for the engagement rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e771e9",
   "metadata": {},
   "source": [
    "Calculating the number of dips in GSR and spikes in Heart Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad65c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Function to reverse the signal for finding dips as peaks\n",
    "def find_dips(data):\n",
    "    reversed_data = -1 * data  # Multiply by -1 to reverse\n",
    "    dips, _ = find_peaks(reversed_data)\n",
    "    return dips\n",
    "\n",
    "# Parameters\n",
    "window_before = 10  # Seconds before the event\n",
    "window_after = 10  # Seconds after the event\n",
    "\n",
    "# Initialize counters\n",
    "gsr_dips_count = 0\n",
    "hr_peaks_count = 0\n",
    "\n",
    "# Iterate over each kill event\n",
    "kill_times = df_player1['time'][df_player1['kill'] > 0]\n",
    "\n",
    "for kill_time in kill_times:\n",
    "    start_time = kill_time - window_before\n",
    "    end_time = kill_time + window_after\n",
    "    \n",
    "    # Extract segment around the kill event\n",
    "    segment = df_player1[(df_player1['time'] >= start_time) & (df_player1['time'] <= end_time)]\n",
    "    \n",
    "    # Check if segment is long enough\n",
    "    if len(segment) < (window_before + window_after + 1):\n",
    "        continue\n",
    "    \n",
    "    # GSR dips\n",
    "    gsr_data = segment['gsr'].values\n",
    "    gsr_dips = find_dips(gsr_data)\n",
    "    gsr_dips_count += len(gsr_dips)\n",
    "    \n",
    "    # Heart rate peaks\n",
    "    hr_data = segment['heart_rate'].values\n",
    "    hr_peaks, _ = find_peaks(hr_data)\n",
    "    hr_peaks_count += len(hr_peaks)\n",
    "\n",
    "print(f'Number of GSR dips around kills: {gsr_dips_count}')\n",
    "print(f'Number of heart rate peaks around kills: {hr_peaks_count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458e010a",
   "metadata": {},
   "source": [
    "## Finding the p-value via T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc2c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Function to reverse the signal for finding dips as peaks\n",
    "def find_dips(data):\n",
    "    reversed_data = -1 * data  # Multiply by -1 to reverse\n",
    "    dips, _ = find_peaks(reversed_data)\n",
    "    return dips\n",
    "\n",
    "# Function to extract segments not around kill events for baseline comparison\n",
    "def extract_baseline_segments(df, event_column, time_column, window):\n",
    "    baseline_segments = []\n",
    "    event_times = df[time_column][df[event_column] > 0].values\n",
    "    for _ in range(len(event_times)):\n",
    "        while True:\n",
    "            random_time = np.random.uniform(df[time_column].min() + window, df[time_column].max() - window)\n",
    "            if all(abs(random_time - et) > window for et in event_times):\n",
    "                start_time = random_time - window\n",
    "                end_time = random_time + window\n",
    "                segment = df[(df[time_column] >= start_time) & (df[time_column] <= end_time)]\n",
    "                baseline_segments.append(segment)\n",
    "                break\n",
    "    return baseline_segments\n",
    "\n",
    "window_before = 10  # Seconds before the event\n",
    "window_after = 10  # Seconds after the event\n",
    "\n",
    "# Initialize counters\n",
    "gsr_dips_count = 0\n",
    "hr_peaks_count = 0\n",
    "\n",
    "kill_segments = []\n",
    "baseline_segments = extract_baseline_segments(df_player1, 'kill', 'time', window_before)\n",
    "\n",
    "# Process kill segments\n",
    "kill_times = df_player1['time'][df_player1['kill'] > 0]\n",
    "for kill_time in kill_times:\n",
    "    start_time = kill_time - window_before\n",
    "    end_time = kill_time + window_after\n",
    "    segment = df_player1[(df_player1['time'] >= start_time) & (df_player1['time'] <= end_time)]\n",
    "    kill_segments.append(segment)\n",
    "    if len(segment) < (window_before + window_after + 1):\n",
    "        continue\n",
    "    gsr_dips_count += len(find_dips(segment['gsr'].values))\n",
    "    hr_peaks_count += len(find_peaks(segment['heart_rate'].values)[0])\n",
    "\n",
    "# Process baseline segments for counting\n",
    "baseline_gsr_dips_count = 0\n",
    "baseline_hr_peaks_count = 0\n",
    "for segment in baseline_segments:\n",
    "    baseline_gsr_dips_count += len(find_dips(segment['gsr'].values))\n",
    "    baseline_hr_peaks_count += len(find_peaks(segment['heart_rate'].values)[0])\n",
    "\n",
    "# Calculate averages and handle division by zero\n",
    "kill_gsr_dips_avg = gsr_dips_count / len(kill_segments) if len(kill_segments) > 0 else None\n",
    "kill_hr_peaks_avg = hr_peaks_count / len(kill_segments) if len(kill_segments) > 0 else None\n",
    "\n",
    "baseline_gsr_dips_avg = baseline_gsr_dips_count / len(baseline_segments) if len(baseline_segments) > 0 else None\n",
    "baseline_hr_peaks_avg = baseline_hr_peaks_count / len(baseline_segments) if len(baseline_segments) > 0 else None\n",
    "\n",
    "# Perform statistical tests if valid\n",
    "if kill_gsr_dips_avg is not None and baseline_gsr_dips_avg is not None:\n",
    "    gsr_p_value = ttest_ind([len(find_dips(seg['gsr'].values)) for seg in kill_segments],\n",
    "                            [len(find_dips(seg['gsr'].values)) for seg in baseline_segments], nan_policy='omit').pvalue\n",
    "else:\n",
    "    gsr_p_value = None\n",
    "\n",
    "if kill_hr_peaks_avg is not None and baseline_hr_peaks_avg is not None:\n",
    "    hr_p_value = ttest_ind([len(find_peaks(seg['heart_rate'].values)[0]) for seg in kill_segments],\n",
    "                           [len(find_peaks(seg['heart_rate'].values)[0]) for seg in baseline_segments], nan_policy='omit').pvalue\n",
    "else:\n",
    "    hr_p_value = None\n",
    "\n",
    "print(f'Average GSR dips around kills: {kill_gsr_dips_avg} vs. baseline: {baseline_gsr_dips_avg}')\n",
    "print(f'Average HR peaks around kills: {kill_hr_peaks_avg} vs. baseline: {baseline_hr_peaks_avg}')\n",
    "print(f'GSR Dips Change Significance p-value: {gsr_p_value}')\n",
    "print(f'HR Peaks Change Significance p-value: {hr_p_value}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13b32a3",
   "metadata": {},
   "source": [
    "## P- Value Example and Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fe0dc1",
   "metadata": {},
   "source": [
    "The p-value is like a tool that helps you decide if the differences you observe (like test scores between two groups) are strong enough to say something meaningful, or if they could just be random fluctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989298c9",
   "metadata": {},
   "source": [
    "## Group A (New Method): Scores = 90, 92, 95, 88, 96\n",
    "    \n",
    "  \n",
    "  \n",
    "## Group B (Traditional Method): Scores = 85, 87, 84, 88, 86\n",
    "  \n",
    "  Group A did better - actually intelligent or just luck ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01256f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_segment_peaks_dips(segment, title):\n",
    "    # Time values could be relative indices if 'time' is not in the segment\n",
    "    time_indices = range(len(segment))\n",
    "    \n",
    "    # GSR plot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(time_indices, segment['gsr'], label='GSR', color='blue')\n",
    "    gsr_dips_indices = find_dips(segment['gsr'].values)\n",
    "    plt.scatter(gsr_dips_indices, segment['gsr'].iloc[gsr_dips_indices], color='red', label='Dips')\n",
    "    plt.title(f'GSR with Dips - {title}')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('GSR')\n",
    "    plt.legend()\n",
    "\n",
    "    # Heart Rate plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(time_indices, segment['heart_rate'], label='Heart Rate', color='green')\n",
    "    hr_peaks_indices, _ = find_peaks(segment['heart_rate'].values)\n",
    "    plt.scatter(hr_peaks_indices, segment['heart_rate'].iloc[hr_peaks_indices], color='orange', label='Peaks')\n",
    "    plt.title(f'Heart Rate with Peaks - {title}')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Heart Rate')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Select a representative segment for each condition (around kills and baseline) for visualization\n",
    "if kill_segments:\n",
    "    plot_segment_peaks_dips(kill_segments[0], 'Around Kills')\n",
    "\n",
    "if baseline_segments:\n",
    "    plot_segment_peaks_dips(baseline_segments[0], 'Baseline')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77fd2c1",
   "metadata": {},
   "source": [
    "## Analysing other CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc8e419",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Function to reverse the signal for finding dips as peaks\n",
    "def find_dips(data):\n",
    "    reversed_data = -1 * data  # Multiply by -1 to reverse\n",
    "    dips, _ = find_peaks(reversed_data)\n",
    "    return dips\n",
    "\n",
    "# Parameters\n",
    "window_before = 10  # Seconds before the event\n",
    "window_after = 10  # Seconds after the event\n",
    "\n",
    "# Define the base directory where CSV files are stored\n",
    "base_directory = '/Users/narenkhatwani/Desktop/is_script/augmented_match_data/'\n",
    "\n",
    "# List of matches with full paths\n",
    "matches = [\n",
    "    base_directory + 'match_1_augmented.csv',\n",
    "    base_directory + 'match_2_augmented.csv',\n",
    "    base_directory + 'match_3_augmented.csv',\n",
    "    base_directory + 'match_4_augmented.csv',\n",
    "    base_directory + 'match_5_augmented.csv',\n",
    "    base_directory + 'match_6_augmented.csv'\n",
    "]\n",
    "\n",
    "# Initialize dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Iterate over each match file\n",
    "for match in matches:\n",
    "    df = pd.read_csv(match)\n",
    "    \n",
    "    # Iterate over each player\n",
    "    for player in df['player_id'].unique():\n",
    "        df_player = df[df['player_id'] == player]\n",
    "        \n",
    "        # Initialize counters\n",
    "        gsr_dips_count = 0\n",
    "        hr_peaks_count = 0\n",
    "        \n",
    "        # Check for missing data\n",
    "        if 'gsr/gsr' not in df_player.columns or df_player['gsr/gsr'].isnull().any():\n",
    "            print(f'Missing GSR data for player {player} in {match}')\n",
    "            continue\n",
    "        if 'heart_rate/heart_rate_y' not in df_player.columns or df_player['heart_rate/heart_rate_y'].isnull().any():\n",
    "            print(f'Missing heart rate data for player {player} in {match}')\n",
    "            continue\n",
    "        \n",
    "        # Iterate over each kill event\n",
    "        kill_times = df_player['time'][df_player['kill'] > 0]\n",
    "\n",
    "        for kill_time in kill_times:\n",
    "            start_time = kill_time - window_before\n",
    "            end_time = kill_time + window_after\n",
    "            \n",
    "            # Extract segment around the kill event\n",
    "            segment = df_player[(df_player['time'] >= start_time) & (df_player['time'] <= end_time)]\n",
    "            \n",
    "            # Check if segment is long enough\n",
    "            if len(segment) < (window_before + window_after + 1):\n",
    "                continue\n",
    "            \n",
    "            # GSR dips\n",
    "            gsr_data = segment['gsr/gsr'].values\n",
    "            gsr_dips = find_dips(gsr_data)\n",
    "            gsr_dips_count += len(gsr_dips)\n",
    "            \n",
    "            # Heart rate peaks\n",
    "            hr_data = segment['heart_rate/heart_rate_y'].values\n",
    "            hr_peaks, _ = find_peaks(hr_data)\n",
    "            hr_peaks_count += len(hr_peaks)\n",
    "\n",
    "        # Store results for each player\n",
    "        results[(match, player)] = (gsr_dips_count, hr_peaks_count)\n",
    "\n",
    "# Print final results\n",
    "for key, value in results.items():\n",
    "    print(f'{key[0]} - Player {key[1]}: Number of GSR dips around kills = {value[0]}, Number of heart rate peaks around kills = {value[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba5c971",
   "metadata": {},
   "source": [
    "### Table of Missing Heart Rate Data\n",
    "\n",
    "| Match Number | Players with Missing Heart Rate Data |\n",
    "|--------------|-------------------------------------|\n",
    "| Match 2      | Players 0-9                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5ece46",
   "metadata": {},
   "source": [
    "# Finding the p value for all the players for all the matches from 1-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a69a6",
   "metadata": {},
   "source": [
    "### Match Proficiency Levels\n",
    "\n",
    "| Match Number | Proficiency Level |\n",
    "|--------------|-------------------|\n",
    "| Match 1      | Beginner          |\n",
    "| Match 2      | Beginner          |\n",
    "| Match 3      | Beginners         |\n",
    "| Match 4      | Pros              |\n",
    "| Match 5      | Pros              |\n",
    "| Match 6      | Pros              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd623df",
   "metadata": {},
   "source": [
    "#### Threshold Window set to 10 seconds before and 10 seconds after the kill event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aab3e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Function to reverse the signal for finding dips as peaks\n",
    "def find_dips(data):\n",
    "    reversed_data = -1 * data  # Multiply by -1 to reverse\n",
    "    dips, _ = find_peaks(reversed_data)\n",
    "    return dips\n",
    "\n",
    "# Function to extract baseline segments not around kill events\n",
    "def extract_baseline_segments(df, event_column, time_column, window):\n",
    "    baseline_segments = []\n",
    "    event_times = df[time_column][df[event_column] > 0].values\n",
    "    for _ in range(len(event_times)):\n",
    "        while True:\n",
    "            random_time = np.random.uniform(df[time_column].min() + window, df[time_column].max() - window)\n",
    "            if all(abs(random_time - et) > window for et in event_times):\n",
    "                start_time = random_time - window\n",
    "                end_time = random_time + window\n",
    "                segment = df[(df[time_column] >= start_time) & (df[time_column] <= end_time)]\n",
    "                baseline_segments.append(segment)\n",
    "                break\n",
    "    return baseline_segments\n",
    "\n",
    "# Define the base directory where CSV files are stored\n",
    "base_directory = '/Users/narenkhatwani/Desktop/is_script/augmented_match_data/'\n",
    "\n",
    "# List of matches with full paths\n",
    "matches = [\n",
    "    base_directory + 'match_1_augmented.csv',\n",
    "    base_directory + 'match_2_augmented.csv',\n",
    "    base_directory + 'match_3_augmented.csv',\n",
    "    base_directory + 'match_4_augmented.csv',\n",
    "    base_directory + 'match_5_augmented.csv',\n",
    "    base_directory + 'match_6_augmented.csv'\n",
    "    \n",
    "]\n",
    "\n",
    "window_before = 10  # Seconds before the event\n",
    "window_after = 10  # Seconds after the event\n",
    "\n",
    "# Process each match and each player\n",
    "for match in matches:\n",
    "    df = pd.read_csv(match)\n",
    "    results = []\n",
    "\n",
    "    for player in df['player_id'].unique():\n",
    "        df_player = df[df['player_id'] == player]\n",
    "\n",
    "        if 'gsr/gsr' not in df_player.columns or 'heart_rate/heart_rate_y' not in df_player.columns:\n",
    "            print(f'Missing necessary data for player {player} in {match}')\n",
    "            continue\n",
    "\n",
    "        kill_segments = []\n",
    "        baseline_segments = extract_baseline_segments(df_player, 'kill', 'time', window_before)\n",
    "        \n",
    "        kill_times = df_player['time'][df_player['kill'] > 0]\n",
    "        for kill_time in kill_times:\n",
    "            start_time = kill_time - window_before\n",
    "            end_time = kill_time + window_after\n",
    "            segment = df_player[(df_player['time'] >= start_time) & (df_player['time'] <= end_time)]\n",
    "            kill_segments.append(segment)\n",
    "\n",
    "        # Calculate dips and peaks for kill and baseline segments\n",
    "        kill_gsr_dips = [len(find_dips(seg['gsr/gsr'].values)) for seg in kill_segments]\n",
    "        kill_hr_peaks = [len(find_peaks(seg['heart_rate/heart_rate_y'].values)[0]) for seg in kill_segments]\n",
    "\n",
    "        baseline_gsr_dips = [len(find_dips(seg['gsr/gsr'].values)) for seg in baseline_segments]\n",
    "        baseline_hr_peaks = [len(find_peaks(seg['heart_rate/heart_rate_y'].values)[0]) for seg in baseline_segments]\n",
    "\n",
    "        # Statistical tests\n",
    "        gsr_p_value = ttest_ind(kill_gsr_dips, baseline_gsr_dips, nan_policy='omit').pvalue\n",
    "        hr_p_value = ttest_ind(kill_hr_peaks, baseline_hr_peaks, nan_policy='omit').pvalue\n",
    "\n",
    "        results.append((player, np.mean(kill_gsr_dips), np.mean(kill_hr_peaks), np.mean(baseline_gsr_dips), np.mean(baseline_hr_peaks), gsr_p_value, hr_p_value))\n",
    "\n",
    "    for result in results:\n",
    "        player, k_gsr_avg, k_hr_avg, b_gsr_avg, b_hr_avg, g_pval, h_pval = result\n",
    "        print(f'Match: {match} - Player: {player}')\n",
    "        print(f'  Average GSR dips around kills: {k_gsr_avg} vs. baseline: {b_gsr_avg}')\n",
    "        print(f'  Average HR peaks around kills: {k_hr_avg} vs. baseline: {b_hr_avg}')\n",
    "        print(f'  GSR Dips Change Significance (p-value): {g_pval} - {\"Significant\" if g_pval < 0.05 else \"Not significant\"}')\n",
    "        print(f'  HR Peaks Change Significance (p-value): {h_pval} - {\"Significant\" if h_pval < 0.05 else \"Not significant\"}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee49c9b7",
   "metadata": {},
   "source": [
    "## Significant GSR Dips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf755f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Function to reverse the signal for finding dips as peaks\n",
    "def find_dips(data):\n",
    "    reversed_data = -1 * data  # Multiply by -1 to reverse\n",
    "    dips, _ = find_peaks(reversed_data)\n",
    "    return dips\n",
    "\n",
    "# Function to extract baseline segments not around kill events\n",
    "def extract_baseline_segments(df, event_column, time_column, window):\n",
    "    baseline_segments = []\n",
    "    event_times = df[time_column][df[event_column] > 0].values\n",
    "    for _ in range(len(event_times)):\n",
    "        while True:\n",
    "            random_time = np.random.uniform(df[time_column].min() + window, df[time_column].max() - window)\n",
    "            if all(abs(random_time - et) > window for et in event_times):\n",
    "                start_time = random_time - window\n",
    "                end_time = random_time + window\n",
    "                segment = df[(df[time_column] >= start_time) & (df[time_column] <= end_time)]\n",
    "                baseline_segments.append(segment)\n",
    "                break\n",
    "    return baseline_segments\n",
    "\n",
    "# Define the base directory where CSV files are stored\n",
    "base_directory = '/Users/narenkhatwani/Desktop/is_script/augmented_match_data/'\n",
    "\n",
    "# List of matches with full paths\n",
    "matches = [\n",
    "    base_directory + 'match_1_augmented.csv',\n",
    "    base_directory + 'match_2_augmented.csv',\n",
    "    base_directory + 'match_3_augmented.csv',\n",
    "    base_directory + 'match_4_augmented.csv',\n",
    "    base_directory + 'match_5_augmented.csv',\n",
    "    base_directory + 'match_6_augmented.csv'\n",
    "]\n",
    "\n",
    "window_before = 10  # Seconds before the event\n",
    "window_after = 10  # Seconds after the event\n",
    "\n",
    "# Process each match and each player\n",
    "for match in matches:\n",
    "    df = pd.read_csv(match)\n",
    "    results = []\n",
    "\n",
    "    for player in df['player_id'].unique():\n",
    "        df_player = df[df['player_id'] == player]\n",
    "\n",
    "        if 'gsr/gsr' not in df_player.columns or 'heart_rate/heart_rate_y' not in df_player.columns:\n",
    "            print(f'Missing necessary data for player {player} in {match}')\n",
    "            continue\n",
    "\n",
    "        kill_segments = []\n",
    "        baseline_segments = extract_baseline_segments(df_player, 'kill', 'time', window_before)\n",
    "        \n",
    "        kill_times = df_player['time'][df_player['kill'] > 0]\n",
    "        for kill_time in kill_times:\n",
    "            start_time = kill_time - window_before\n",
    "            end_time = kill_time + window_after\n",
    "            segment = df_player[(df_player['time'] >= start_time) & (df_player['time'] <= end_time)]\n",
    "            kill_segments.append(segment)\n",
    "\n",
    "        # Calculate dips and peaks for kill and baseline segments\n",
    "        kill_gsr_dips = [len(find_dips(seg['gsr/gsr'].values)) for seg in kill_segments]\n",
    "        kill_hr_peaks = [len(find_peaks(seg['heart_rate/heart_rate_y'].values)[0]) for seg in kill_segments]\n",
    "\n",
    "        baseline_gsr_dips = [len(find_dips(seg['gsr/gsr'].values)) for seg in baseline_segments]\n",
    "        baseline_hr_peaks = [len(find_peaks(seg['heart_rate/heart_rate_y'].values)[0]) for seg in baseline_segments]\n",
    "\n",
    "        # Statistical tests\n",
    "        gsr_p_value = ttest_ind(kill_gsr_dips, baseline_gsr_dips, nan_policy='omit').pvalue\n",
    "        hr_p_value = ttest_ind(kill_hr_peaks, baseline_hr_peaks, nan_policy='omit').pvalue\n",
    "\n",
    "        results.append((player, np.mean(kill_gsr_dips), np.mean(kill_hr_peaks), np.mean(baseline_gsr_dips), np.mean(baseline_hr_peaks), gsr_p_value, hr_p_value))\n",
    "\n",
    "    for result in results:\n",
    "        player, k_gsr_avg, k_hr_avg, b_gsr_avg, b_hr_avg, g_pval, h_pval = result\n",
    "        if g_pval < 0.05 or h_pval < 0.05:\n",
    "            print(f'Match: {match} - Player: {player}')\n",
    "            if g_pval < 0.05:\n",
    "                print(f'  Average GSR dips around kills: {k_gsr_avg} vs. baseline: {b_gsr_avg} - Significant (p-value: {g_pval})')\n",
    "            if h_pval < 0.05:\n",
    "                print(f'  Average HR peaks around kills: {k_hr_avg} vs. baseline: {b_hr_avg} - Significant (p-value: {h_pval})')\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e29c9c",
   "metadata": {},
   "source": [
    "## Significant Heart Rate Spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d4164",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Function to reverse the signal for finding dips as peaks\n",
    "def find_dips(data):\n",
    "    reversed_data = -1 * data  # Multiply by -1 to reverse\n",
    "    dips, _ = find_peaks(reversed_data)\n",
    "    return dips\n",
    "\n",
    "# Function to extract baseline segments not around kill events\n",
    "def extract_baseline_segments(df, event_column, time_column, window):\n",
    "    baseline_segments = []\n",
    "    event_times = df[time_column][df[event_column] > 0].values\n",
    "    for _ in range(len(event_times)):\n",
    "        while True:\n",
    "            random_time = np.random.uniform(df[time_column].min() + window, df[time_column].max() - window)\n",
    "            if all(abs(random_time - et) > window for et in event_times):\n",
    "                start_time = random_time - window\n",
    "                end_time = random_time + window\n",
    "                segment = df[(df[time_column] >= start_time) & (df[time_column] <= end_time)]\n",
    "                baseline_segments.append(segment)\n",
    "                break\n",
    "    return baseline_segments\n",
    "\n",
    "# Define the base directory where CSV files are stored\n",
    "base_directory = '/Users/narenkhatwani/Desktop/is_script/augmented_match_data/'\n",
    "\n",
    "# List of matches with full paths\n",
    "matches = [\n",
    "    base_directory + 'match_1_augmented.csv',\n",
    "    base_directory + 'match_2_augmented.csv',\n",
    "    base_directory + 'match_3_augmented.csv',\n",
    "    base_directory + 'match_4_augmented.csv',\n",
    "    base_directory + 'match_5_augmented.csv',\n",
    "    base_directory + 'match_6_augmented.csv'\n",
    "]\n",
    "\n",
    "window_before = 10  # Seconds before the event\n",
    "window_after = 10  # Seconds after the event\n",
    "\n",
    "# Process each match and each player\n",
    "for match in matches:\n",
    "    df = pd.read_csv(match)\n",
    "    results = []\n",
    "\n",
    "    for player in df['player_id'].unique():\n",
    "        df_player = df[df['player_id'] == player]\n",
    "\n",
    "        if 'gsr/gsr' not in df_player.columns or 'heart_rate/heart_rate_y' not in df_player.columns:\n",
    "            print(f'Missing necessary data for player {player} in {match}')\n",
    "            continue\n",
    "\n",
    "        kills_count = df_player['kill'].sum()  # Sum of kills for the player\n",
    "        assists_count = df_player['assist'].sum()  # Sum of assists for the player\n",
    "\n",
    "        kill_segments = []\n",
    "        baseline_segments = extract_baseline_segments(df_player, 'kill', 'time', window_before)\n",
    "        \n",
    "        kill_times = df_player['time'][df_player['kill'] > 0]\n",
    "        for kill_time in kill_times:\n",
    "            start_time = kill_time - window_before\n",
    "            end_time = kill_time + window_after\n",
    "            segment = df_player[(df_player['time'] >= start_time) & (df_player['time'] <= end_time)]\n",
    "            kill_segments.append(segment)\n",
    "\n",
    "        # Calculate dips and peaks for kill and baseline segments\n",
    "        kill_gsr_dips = [len(find_dips(seg['gsr/gsr'].values)) for seg in kill_segments]\n",
    "        kill_hr_peaks = [len(find_peaks(seg['heart_rate/heart_rate_y'].values)[0]) for seg in kill_segments]\n",
    "\n",
    "        baseline_gsr_dips = [len(find_dips(seg['gsr/gsr'].values)) for seg in baseline_segments]\n",
    "        baseline_hr_peaks = [len(find_peaks(seg['heart_rate/heart_rate_y'].values)[0]) for seg in baseline_segments]\n",
    "\n",
    "        # Statistical tests\n",
    "        gsr_p_value = ttest_ind(kill_gsr_dips, baseline_gsr_dips, nan_policy='omit').pvalue\n",
    "        hr_p_value = ttest_ind(kill_hr_peaks, baseline_hr_peaks, nan_policy='omit').pvalue\n",
    "\n",
    "        results.append((player, kills_count, assists_count, np.mean(kill_gsr_dips), np.mean(kill_hr_peaks), np.mean(baseline_gsr_dips), np.mean(baseline_hr_peaks), gsr_p_value, hr_p_value))\n",
    "\n",
    "    for result in results:\n",
    "        player, kills, assists, k_gsr_avg, k_hr_avg, b_gsr_avg, b_hr_avg, g_pval, h_pval = result\n",
    "        if g_pval < 0.05 or h_pval < 0.05:\n",
    "            print(f'Match: {match} - Player: {player} - Kills: {kills}, Assists: {assists}')\n",
    "            if g_pval < 0.05:\n",
    "                print(f'  Average GSR dips around kills: {k_gsr_avg} vs. baseline: {b_gsr_avg} - Significant (p-value: {g_pval})')\n",
    "            if h_pval < 0.05:\n",
    "                print(f'  Average HR peaks around kills: {k_hr_avg} vs. baseline: {b_hr_avg} - Significant (p-value: {h_pval})')\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8086d1",
   "metadata": {},
   "source": [
    "## Above here I found that not many significant cases were found "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5d44ba",
   "metadata": {},
   "source": [
    "*So, I changed the hypothesis of the threshold of the data being considered to 45 seconds before the kill and 10 second after the kill*\n",
    "\n",
    "### Mainly because the GSR dips and Rise and Heart rate for League of Legends would occur before the kills "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407486dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Function to reverse the signal for finding dips as peaks\n",
    "def find_dips(data):\n",
    "    reversed_data = -1 * data  # Multiply by -1 to reverse\n",
    "    dips, _ = find_peaks(reversed_data)\n",
    "    return dips\n",
    "\n",
    "# Function to extract baseline segments not around kill events\n",
    "def extract_baseline_segments(df, event_column, time_column, window):\n",
    "    baseline_segments = []\n",
    "    event_times = df[time_column][df[event_column] > 0].values\n",
    "    for _ in range(len(event_times)):\n",
    "        while True:\n",
    "            random_time = np.random.uniform(df[time_column].min() + window, df[time_column].max() - window)\n",
    "            if all(abs(random_time - et) > window for et in event_times):\n",
    "                start_time = random_time - window\n",
    "                end_time = random_time + window\n",
    "                segment = df[(df[time_column] >= start_time) & (df[time_column] <= end_time)]\n",
    "                baseline_segments.append(segment)\n",
    "                break\n",
    "    return baseline_segments\n",
    "\n",
    "# Define the base directory where CSV files are stored\n",
    "base_directory = '/Users/narenkhatwani/Desktop/is_script/augmented_match_data/'\n",
    "\n",
    "# List of matches with full paths\n",
    "matches = [\n",
    "    base_directory + 'match_1_augmented.csv',\n",
    "    base_directory + 'match_2_augmented.csv',\n",
    "    base_directory + 'match_3_augmented.csv',\n",
    "    base_directory + 'match_4_augmented.csv',\n",
    "    base_directory + 'match_5_augmented.csv',\n",
    "    base_directory + 'match_6_augmented.csv'\n",
    "]\n",
    "\n",
    "window_before = 45  # Seconds before the event\n",
    "window_after =10  # Seconds after the event\n",
    "\n",
    "# Process each match\n",
    "for match in matches:\n",
    "    df = pd.read_csv(match)\n",
    "    total_kills_in_match = df['kill'].sum()  # Calculate total kills in the match\n",
    "    results = []\n",
    "\n",
    "    for player in df['player_id'].unique():\n",
    "        df_player = df[df['player_id'] == player]\n",
    "\n",
    "        if 'gsr/gsr' not in df_player.columns or 'heart_rate/heart_rate_y' not in df_player.columns:\n",
    "            print(f'Missing necessary data for player {player} in {match}')\n",
    "            continue\n",
    "\n",
    "        kills_count = df_player['kill'].sum()  # Sum of kills for the player\n",
    "        assists_count = df_player['assist'].sum()  # Sum of assists for the player\n",
    "\n",
    "        kill_segments = []\n",
    "        baseline_segments = extract_baseline_segments(df_player, 'kill', 'time', window_before)\n",
    "        \n",
    "        kill_times = df_player['time'][df_player['kill'] > 0]\n",
    "        for kill_time in kill_times:\n",
    "            start_time = kill_time - window_before\n",
    "            end_time = kill_time + window_after\n",
    "            segment = df_player[(df_player['time'] >= start_time) & (df_player['time'] <= end_time)]\n",
    "            kill_segments.append(segment)\n",
    "\n",
    "        # Calculate dips and peaks for kill and baseline segments\n",
    "        kill_gsr_dips = [len(find_dips(seg['gsr/gsr'].values)) for seg in kill_segments]\n",
    "        kill_hr_peaks = [len(find_peaks(seg['heart_rate/heart_rate_y'].values)[0]) for seg in kill_segments]\n",
    "\n",
    "        baseline_gsr_dips = [len(find_dips(seg['gsr/gsr'].values)) for seg in baseline_segments]\n",
    "        baseline_hr_peaks = [len(find_peaks(seg['heart_rate/heart_rate_y'].values)[0]) for seg in baseline_segments]\n",
    "\n",
    "        # Statistical tests\n",
    "        gsr_p_value = ttest_ind(kill_gsr_dips, baseline_gsr_dips, nan_policy='omit').pvalue\n",
    "        hr_p_value = ttest_ind(kill_hr_peaks, baseline_hr_peaks, nan_policy='omit').pvalue\n",
    "\n",
    "        results.append((player, kills_count, assists_count, np.mean(kill_gsr_dips), np.mean(kill_hr_peaks), np.mean(baseline_gsr_dips), np.mean(baseline_hr_peaks), gsr_p_value, hr_p_value))\n",
    "\n",
    "    print(f\"Total kills in {match}: {total_kills_in_match}\")\n",
    "    for result in results:\n",
    "        player, kills, assists, k_gsr_avg, k_hr_avg, b_gsr_avg, b_hr_avg, g_pval, h_pval = result\n",
    "        if g_pval < 0.05 or h_pval < 0.05:\n",
    "            print(f'Player: {player} - Kills: {kills}, Assists: {assists}')\n",
    "            if g_pval < 0.05:\n",
    "                print(f'  Average GSR dips around kills: {k_gsr_avg} vs. baseline: {b_gsr_avg} - Significant (p-value: {g_pval})')\n",
    "            if h_pval < 0.05:\n",
    "                print(f'  Average HR peaks around kills: {k_hr_avg} vs. baseline: {b_hr_avg} - Significant (p-value: {h_pval})')\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1de55d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Function to reverse the signal for finding dips as peaks\n",
    "def find_dips(data):\n",
    "    reversed_data = -1 * data  # Multiply by -1 to reverse\n",
    "    dips, _ = find_peaks(reversed_data)\n",
    "    return dips\n",
    "\n",
    "# Function to extract baseline segments not around kill events\n",
    "def extract_baseline_segments(df, event_column, time_column, window):\n",
    "    baseline_segments = []\n",
    "    event_times = df[time_column][df[event_column] > 0].values\n",
    "    for _ in range(len(event_times)):\n",
    "        while True:\n",
    "            random_time = np.random.uniform(df[time_column].min() + window, df[time_column].max() - window)\n",
    "            if all(abs(random_time - et) > window for et in event_times):\n",
    "                start_time = random_time - window\n",
    "                end_time = random_time + window\n",
    "                segment = df[(df[time_column] >= start_time) & (df[time_column] <= end_time)]\n",
    "                baseline_segments.append(segment)\n",
    "                break\n",
    "    return baseline_segments\n",
    "\n",
    "# Define the base directory where CSV files are stored\n",
    "base_directory = '/Users/narenkhatwani/Desktop/is_script/augmented_match_data/'\n",
    "\n",
    "# List of matches with full paths\n",
    "matches = [\n",
    "    base_directory + 'match_1_augmented.csv',\n",
    "    base_directory + 'match_2_augmented.csv',\n",
    "    base_directory + 'match_3_augmented.csv',\n",
    "    base_directory + 'match_4_augmented.csv',\n",
    "    base_directory + 'match_5_augmented.csv',\n",
    "    base_directory + 'match_6_augmented.csv'\n",
    "]\n",
    "\n",
    "window_before = 10  # Seconds before the event\n",
    "window_after = 10  # Seconds after the event\n",
    "\n",
    "# DataFrame to store all results\n",
    "all_results = []\n",
    "\n",
    "# Process each match and each player\n",
    "for match in matches:\n",
    "    df = pd.read_csv(match)\n",
    "    match_results = []\n",
    "\n",
    "    for player in df['player_id'].unique():\n",
    "        df_player = df[df['player_id'] == player]\n",
    "\n",
    "        if 'gsr/gsr' not in df_player.columns or 'heart_rate/heart_rate_y' not in df_player.columns:\n",
    "            continue\n",
    "\n",
    "        kill_segments = []\n",
    "        baseline_segments = extract_baseline_segments(df_player, 'kill', 'time', window_before)\n",
    "        \n",
    "        kill_times = df_player['time'][df_player['kill'] > 0]\n",
    "        for kill_time in kill_times:\n",
    "            start_time = kill_time - window_before\n",
    "            end_time = kill_time + window_after\n",
    "            segment = df_player[(df_player['time'] >= start_time) & (df_player['time'] <= end_time)]\n",
    "            kill_segments.append(segment)\n",
    "\n",
    "        kill_gsr_dips = [len(find_dips(seg['gsr/gsr'].values)) for seg in kill_segments]\n",
    "        kill_hr_peaks = [len(find_peaks(seg['heart_rate/heart_rate_y'].values)[0]) for seg in kill_segments]\n",
    "\n",
    "        baseline_gsr_dips = [len(find_dips(seg['gsr/gsr'].values)) for seg in baseline_segments]\n",
    "        baseline_hr_peaks = [len(find_peaks(seg['heart_rate/heart_rate_y'].values)[0]) for seg in baseline_segments]\n",
    "\n",
    "        gsr_p_value = ttest_ind(kill_gsr_dips, baseline_gsr_dips, nan_policy='omit').pvalue\n",
    "        hr_p_value = ttest_ind(kill_hr_peaks, baseline_hr_peaks, nan_policy='omit').pvalue\n",
    "\n",
    "        total_kills = df_player['kill'].sum()  # Assuming 'kill' column holds kill counts\n",
    "\n",
    "        match_results.append({\n",
    "            'Player': player,\n",
    "            'Match': match.split('/')[-1],  # Get just the file name\n",
    "            'Avg GSR Dips (Kills)': np.mean(kill_gsr_dips),\n",
    "            'Avg GSR Dips (Baseline)': np.mean(baseline_gsr_dips),\n",
    "            'Avg HR Peaks (Kills)': np.mean(kill_hr_peaks),\n",
    "            'Avg HR Peaks (Baseline)': np.mean(baseline_hr_peaks),\n",
    "            'GSR P-value': gsr_p_value,\n",
    "            'HR P-value': hr_p_value,\n",
    "            'Total Kills': total_kills\n",
    "        })\n",
    "\n",
    "    if match_results:\n",
    "        # Append the DataFrame of this match's results to the list\n",
    "        all_results.append(pd.DataFrame(match_results))\n",
    "\n",
    "# Concatenate all DataFrames if not empty\n",
    "if all_results:\n",
    "    final_results_df = pd.concat(all_results, ignore_index=True)\n",
    "    print(final_results_df)\n",
    "else:\n",
    "    print(\"No data available to display.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4ad18c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debfeee9",
   "metadata": {},
   "source": [
    "## Finally I condiered the Threshold as:\n",
    "\n",
    "### window_before = 35  # Seconds before the event\n",
    "### window_after = 15  # Seconds after the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c057dae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Function to reverse the signal for finding dips as peaks\n",
    "def find_dips(data):\n",
    "    reversed_data = -1 * data  # Multiply by -1 to reverse\n",
    "    dips, _ = find_peaks(reversed_data)\n",
    "    return dips\n",
    "\n",
    "# Function to extract baseline segments not around kill events\n",
    "def extract_baseline_segments(df, event_column, time_column, window):\n",
    "    baseline_segments = []\n",
    "    event_times = df[time_column][df[event_column] > 0].values\n",
    "    for _ in range(len(event_times)):\n",
    "        while True:\n",
    "            random_time = np.random.uniform(df[time_column].min() + window, df[time_column].max() - window)\n",
    "            if all(abs(random_time - et) > window for et in event_times):\n",
    "                start_time = random_time - window\n",
    "                end_time = random_time + window\n",
    "                segment = df[(df[time_column] >= start_time) & (df[time_column] <= end_time)]\n",
    "                baseline_segments.append(segment)\n",
    "                break\n",
    "    return baseline_segments\n",
    "\n",
    "# Define the base directory where CSV files are stored\n",
    "base_directory = '/Users/narenkhatwani/Desktop/is_script/augmented_match_data/'\n",
    "\n",
    "# List of matches with full paths\n",
    "matches = [\n",
    "    base_directory + 'match_1_augmented.csv',\n",
    "    base_directory + 'match_2_augmented.csv',\n",
    "    base_directory + 'match_3_augmented.csv',\n",
    "    base_directory + 'match_4_augmented.csv',\n",
    "    base_directory + 'match_5_augmented.csv',\n",
    "    base_directory + 'match_6_augmented.csv'\n",
    "]\n",
    "\n",
    "window_before = 35  # Seconds before the event\n",
    "window_after = 15  # Seconds after the event\n",
    "\n",
    "# DataFrame to store all results\n",
    "all_results = []\n",
    "\n",
    "# Process each match and each player\n",
    "for match in matches:\n",
    "    df = pd.read_csv(match)\n",
    "    match_results = []\n",
    "\n",
    "    for player in df['player_id'].unique():\n",
    "        df_player = df[df['player_id'] == player]\n",
    "\n",
    "        if 'gsr/gsr' not in df_player.columns or 'heart_rate/heart_rate_y' not in df_player.columns:\n",
    "            continue\n",
    "\n",
    "        kill_segments = []\n",
    "        baseline_segments = extract_baseline_segments(df_player, 'kill', 'time', window_before)\n",
    "        \n",
    "        kill_times = df_player['time'][df_player['kill'] > 0]\n",
    "        for kill_time in kill_times:\n",
    "            start_time = kill_time - window_before\n",
    "            end_time = kill_time + window_after\n",
    "            segment = df_player[(df_player['time'] >= start_time) & (df_player['time'] <= end_time)]\n",
    "            kill_segments.append(segment)\n",
    "\n",
    "        kill_gsr_dips = [len(find_dips(seg['gsr/gsr'].values)) for seg in kill_segments]\n",
    "        kill_hr_peaks = [len(find_peaks(seg['heart_rate/heart_rate_y'].values)[0]) for seg in kill_segments]\n",
    "\n",
    "        baseline_gsr_dips = [len(find_dips(seg['gsr/gsr'].values)) for seg in baseline_segments]\n",
    "        baseline_hr_peaks = [len(find_peaks(seg['heart_rate/heart_rate_y'].values)[0]) for seg in baseline_segments]\n",
    "\n",
    "        gsr_p_value = ttest_ind(kill_gsr_dips, baseline_gsr_dips, nan_policy='omit').pvalue\n",
    "        hr_p_value = ttest_ind(kill_hr_peaks, baseline_hr_peaks, nan_policy='omit').pvalue\n",
    "\n",
    "        total_kills = df_player['kill'].sum()  # Assuming 'kill' column holds kill counts\n",
    "\n",
    "        match_results.append({\n",
    "            'Player': player,\n",
    "            'Match': match.split('/')[-1],  # Get just the file name\n",
    "            'Avg GSR Dips (Kills)': np.mean(kill_gsr_dips),\n",
    "            'Avg GSR Dips (Baseline)': np.mean(baseline_gsr_dips),\n",
    "            'Avg HR Peaks (Kills)': np.mean(kill_hr_peaks),\n",
    "            'Avg HR Peaks (Baseline)': np.mean(baseline_hr_peaks),\n",
    "            'GSR P-value': gsr_p_value,\n",
    "            'HR P-value': hr_p_value,\n",
    "            'Total Kills': total_kills\n",
    "        })\n",
    "\n",
    "    if match_results:\n",
    "        # Append the DataFrame of this match's results to the list\n",
    "        all_results.append(pd.DataFrame(match_results))\n",
    "\n",
    "# Concatenate all DataFrames if not empty\n",
    "if all_results:\n",
    "    final_results_df = pd.concat(all_results, ignore_index=True)\n",
    "    print(final_results_df)\n",
    "else:\n",
    "    print(\"No data available to display.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e70685",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07895c1c",
   "metadata": {},
   "source": [
    "# Purpose of the Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1d4ab3",
   "metadata": {},
   "source": [
    "0.05 threshold is used to determine whether the differences observed in the data (such as the **number of galvanic skin response (GSR) dips** and **heart rate (HR) peaks during kill events** versus baseline periods in a gaming context) are **statistically significant** *or* **could have occurred by luck**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e921830",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364735b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display  # This is used for nicer output in Jupyter notebooks\n",
    "\n",
    "data = final_results_df  # Assuming 'final_results_df' is your initial DataFrame\n",
    "\n",
    "# Define the match names we are interested in (from Match_1 to Match_6)\n",
    "match_names = [f'match_{i}_augmented.csv' for i in range(1, 7)]\n",
    "\n",
    "# Create a dictionary to store DataFrames for each match\n",
    "match_dataframes = {}\n",
    "\n",
    "# Filter and sort data for each specified match\n",
    "for match in match_names:\n",
    "    # Filter the DataFrame for this specific match\n",
    "    match_df = data[data['Match'] == match]\n",
    "\n",
    "    # Drop rows with NaN values\n",
    "    match_df = match_df.dropna()\n",
    "\n",
    "    # Sorting the DataFrame by GSR P-value and then by HR P-value for secondary sorting\n",
    "    if not match_df.empty:\n",
    "        sorted_df = match_df.sort_values(by=['GSR P-value', 'HR P-value'], ascending=[True, True])\n",
    "\n",
    "        # Determine significant parameters based on p-values\n",
    "        def significant_parameters(row):\n",
    "            significant = []\n",
    "            if row['GSR P-value'] < 0.05:\n",
    "                significant.append('GSR')\n",
    "            if row['HR P-value'] < 0.05:\n",
    "                significant.append('HR')\n",
    "            return ', '.join(significant) if significant else 'None'\n",
    "\n",
    "        # Add a new column to indicate significant parameters\n",
    "        sorted_df['Significant Parameters'] = sorted_df.apply(significant_parameters, axis=1)\n",
    "\n",
    "        # Add the sorted DataFrame to the dictionary\n",
    "        match_dataframes[match] = sorted_df\n",
    "\n",
    "# Display each sorted DataFrame for verification\n",
    "for match, df in match_dataframes.items():\n",
    "    print(f\"Sorted DataFrame for {match}:\")\n",
    "    display(df)  # Displaying the DataFrame in a Jupyter Notebook-friendly format\n",
    "    print(\"\\n\")  # Adding a newline for better readability between DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9389ff",
   "metadata": {},
   "source": [
    "## Ignoring any warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e819b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore all runtime warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
